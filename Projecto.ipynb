{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adicionar \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "cwd = os.getcwd() \n",
    "files = os.listdir(cwd)\n",
    "print(\"Files in %r: %s\" % (cwd, files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/catarinatomasr/projectoEDSB/main/HR_DS.csv'\n",
    "s=requests.get(url).content\n",
    "df=pd.read_csv(io.StringIO(s.decode('utf-8')))\n",
    "data = df\n",
    "#df = pd.read_csv('https://github.com/catarinatomasr/projectoEDSB/blob/main/HR_DS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 40)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.replace(regex={'No':0,'Yes':1}, inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definir perguntas para responder\n",
    "Story "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preparation/Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()\n",
    "#Checking the Null Values\n",
    "pd.set_option('display.max_rows', 200)\n",
    "#data.isnull().sum() #no null values\n",
    "#data.info()\n",
    "\n",
    "#check for  duplicate records and have the unique records to be included into your dataset:\n",
    "#data[data.duplicated()] #zero duplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Over18 : ['Y']\n",
    "#Y    1470\n",
    "#so tem um unico valor, por isso vou apagar por ser inutil\n",
    "#data['StandardHours'].unique() tambem so tem 1 valor, que é sempre 80\n",
    "#data['EmployeeCount'].unique() tambem so tem 1 valor, que é 1\n",
    "\n",
    "#não trazem valor \n",
    "\n",
    "data=data.drop('Over18', axis=1)\n",
    "data=data.drop('StandardHours', axis=1)\n",
    "data=data.drop('EmployeeCount', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ver os data types e ver os seus valores unicos\n",
    "\n",
    "for column in data.columns:\n",
    "    if data[column].dtype==object:\n",
    "        print(str(column) + ' : ' + str(data[column].unique()))\n",
    "        print(data[column].value_counts())\n",
    "        print('_______________________________________________________________')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformar as colunas que não sao numericas, em numericas pois os modelos n aceitam colunas texto\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "for column in data.columns:\n",
    "    if data[column].dtype == np.int64:\n",
    "            continue\n",
    "    data[column]=LabelEncoder().fit_transform(data[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turnover_Summary = data.groupby('Attrition')\n",
    "turnover_Summary.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analise descritiva dos dados-Gráficos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attrition em função das variaveis categoricas\n",
    "fig, axes = plt.subplots(5,2, figsize = (30,20))\n",
    "sns.countplot(ax = axes[0,0], data = data, x = 'Age', hue='Attrition',palette='colorblind')\n",
    "sns.countplot(ax = axes[0,1],hue=data.Attrition, x=data.BusinessTravel)\n",
    "sns.countplot(ax = axes[1,0],hue=data.Attrition, x=data.Department)\n",
    "sns.countplot(ax = axes[1,1],hue=data.Attrition, x=data.EducationField)\n",
    "sns.countplot(ax = axes[2,0],hue=data.Attrition, x=data.MaritalStatus)\n",
    "sns.countplot(ax = axes[2,1],hue=data.Attrition, x=data.OverTime)\n",
    "sns.countplot(ax = axes[3,0],hue=data.Attrition, x=data.Gender)\n",
    "sns.countplot(ax = axes[3,1],x=data.Attrition, hue=data.EducationField)\n",
    "sns.countplot(ax = axes[4,0],x='JobRole', hue='Attrition', data=data)\n",
    "sns.histplot(ax = axes[4,1],hue=data.Attrition, x=data.DistanceFromHome)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distribution(var_select, bin_size) : \n",
    "# Calculate the correlation coefficient between the new variable and the target\n",
    "    corr = data['Attrition'].corr(data[var_select])\n",
    "    corr = np.round(corr,3)\n",
    "    tmp1 = attrition[var_select]\n",
    "    tmp2 = no_attrition[var_select]\n",
    "    hist_data = [tmp1, tmp2]\n",
    "    \n",
    "    group_labels = ['Yes_attrition', 'No_attrition']\n",
    "    colors = ['#FFD700', '#7EC0EE']\n",
    "\n",
    "    fig = ff.create_distplot(hist_data, group_labels, colors = colors, show_hist = True, curve_type='kde', bin_size = bin_size)\n",
    "    \n",
    "    fig['layout'].update(title = var_select+' '+'(corr target ='+ str(corr)+')')\n",
    "\n",
    "    py.iplot(fig, filename = 'Density plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def barplot(var_select, x_no_numeric) :\n",
    "    tmp1 = data[(data['Attrition'] != 0)]\n",
    "    tmp2 = data[(data['Attrition'] == 0)]\n",
    "    tmp3 = pd.DataFrame(pd.crosstab(data[var_select],data['Attrition']), )\n",
    "    tmp3['Attr%'] = tmp3[1] / (tmp3[1] + tmp3[0]) * 100\n",
    "    if x_no_numeric == True  : \n",
    "        tmp3 = tmp3.sort_values(1, ascending = True)\n",
    "\n",
    "    color=['green','orange' ]\n",
    "    trace1 = go.Bar(\n",
    "        x=tmp1[var_select].value_counts().keys().tolist(),\n",
    "        y=tmp1[var_select].value_counts().values.tolist(),\n",
    "        name='Yes_Attrition', marker=dict(\n",
    "        color='orange',\n",
    "        line=dict(color='#000000',width=2)))\n",
    "\n",
    "    \n",
    "    trace2 = go.Bar(\n",
    "        x=tmp2[var_select].value_counts().keys().tolist(),\n",
    "        y=tmp2[var_select].value_counts().values.tolist(),\n",
    "        name='No_Attrition', opacity = 0.9, marker=dict(\n",
    "        color='green',\n",
    "        line=dict(color='#000000',width=2)))\n",
    "    \n",
    "    trace3 =  go.Scatter(   \n",
    "        x=tmp3.index,\n",
    "        y=tmp3['Attr%'],\n",
    "        yaxis = 'y2',\n",
    "        name='% Attrition', opacity = 0.9, marker=dict(\n",
    "        color='darkblue',\n",
    "        line=dict(color='#000000',width=0.5\n",
    "        )))\n",
    "\n",
    "    layout = dict(title =  str(var_select),\n",
    "              #width=700,\n",
    "              #height=400, \n",
    "              legend =dict(yanchor = \"top\", y=0.99, xanchor = 'right', x=0.99),\n",
    "              xaxis=dict(), \n",
    "              yaxis=dict(title= 'Count'), \n",
    "              font = dict(size = 24),\n",
    "              yaxis2=dict(range= [-0, 75], \n",
    "                          overlaying= 'y', \n",
    "                          anchor= 'x', \n",
    "                          side= 'right',\n",
    "                          zeroline=False,\n",
    "                          showgrid= False, \n",
    "                          title= '% Attrition'\n",
    "                         ))\n",
    "            \n",
    "\n",
    "    fig = go.Figure(data=[trace1, trace2, trace3], layout=layout)\n",
    "    py.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pie(var_select) :\n",
    "    \n",
    "    colors = ['#B9C0C9', '#CED2FD', '#ceb0ff', 'lightskyblue', 'lightgrey', '#EAB9FC', 'cyan', '#DDC8FE']\n",
    "    trace1 = go.Pie(values  = attrition[var_select].value_counts().values.tolist(),\n",
    "                    labels  = attrition[var_select].value_counts().keys().tolist(),\n",
    "                    textfont=dict(size=15),\n",
    "                    hoverinfo = \"label+percent+name\",\n",
    "                    domain  = dict(x = [0,.48]),\n",
    "                    name    = \"attrition employes\",\n",
    "                    marker  = dict(colors = colors, line = dict(width = 1.5)))\n",
    "    trace2 = go.Pie(values  = no_attrition[var_select].value_counts().values.tolist(),\n",
    "                    labels  = no_attrition[var_select].value_counts().keys().tolist(),\n",
    "                    textfont=dict(size=15), \n",
    "                    hoverinfo = \"label+percent+name\",\n",
    "                    marker  = dict(colors = colors, line = dict(width = 1.5)),\n",
    "                    domain  = dict(x = [.52,1]),\n",
    "                    name    = \"Non attrition employes\" )\n",
    "\n",
    "    layout = go.Layout(dict(title = var_select + \" distribution in employees attrition \",\n",
    "                            annotations = [dict(text = \"Yes_attrition\",\n",
    "                                                font = dict(size = 13),\n",
    "                                                showarrow = False,\n",
    "                                                x = .22, y = -0.1),\n",
    "                                            dict(text = \"No_attrition\",\n",
    "                                                font = dict(size = 13),\n",
    "                                                showarrow = False,\n",
    "                                                x = .8,y = -.1)]))\n",
    "                                          \n",
    "\n",
    "    fig  = go.Figure(data = [trace1,trace2],layout = layout)\n",
    "    py.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attrition = data[(data['Attrition'] != 0)]\n",
    "no_attrition = data[(data['Attrition'] == 0)]\n",
    "\n",
    "trace = go.Pie(labels = ['No_attrition', 'Yes_attrition'], values = data['Attrition'].value_counts(), \n",
    "               textfont=dict(size=15),\n",
    "               marker=dict(colors=['#B9C0C9','yellow'], \n",
    "               line=dict(color='#000000', width=1.5)))\n",
    "layout = dict(title =  'Distribution of attrition variable')          \n",
    "fig = dict(data = [trace], layout=layout)\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barplot('Age', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barplot('DistanceFromHome', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barplot('Department', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barplot('EducationField', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df=data.sort_values(by=\"Attrition\")\n",
    "fig=px.histogram(plot_df, x='MonthlyIncome', color='Attrition', \n",
    "                 opacity=0.8, histnorm='density', barmode='overlay', marginal='box',\n",
    "                 color_discrete_map={'Yes': '#B9C0C9','No': '#CDBBA7'})\n",
    "fig.update_layout(title_text='Distribution of Monthly Income by Attrition Status',\n",
    "                  xaxis_title='Monthly Income, $', yaxis_title='Density',font_color='#28221D',\n",
    "                  paper_bgcolor='#F4F2F0', plot_bgcolor='#F4F2F0', legend_traceorder='reversed')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df=data.sort_values(by=\"Attrition\")\n",
    "fig=px.histogram(plot_df, x='OverTime', color='Attrition', \n",
    "                 opacity=0.8, histnorm='density', barmode='overlay', marginal='box',\n",
    "                 color_discrete_map={'Yes': '#B9C0C9','No': '#CDBBA7'})\n",
    "fig.update_layout(title_text='Distribution of Over Time by Attrition Status',\n",
    "                  xaxis_title='Over Time, ', yaxis_title='Density',font_color='#28221D',\n",
    "                  paper_bgcolor='#F4F2F0', plot_bgcolor='#F4F2F0', legend_traceorder='reversed')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_attrition = data[data['Attrition'] == 1]\n",
    "df_attrition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost calculation \n",
    "# best rate is around 10%. what is the cost of the 6% that left the company, on avg\n",
    "\n",
    "((df_attrition['MonthlyRate'].sum())*3)*0.06/0.16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ver a correlação dos dados\n",
    "plt.figure(figsize=(14,14))\n",
    "sns.heatmap(data.corr(), annot=True, fmt='0.0%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML-Clustering/Unsupervising\n",
    "\n",
    "#Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ec62e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform non-numeric columns into numerical columns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "for column in df.columns:\n",
    "        if df[column].dtype == np.number:\n",
    "            continue\n",
    "        df[column] = LabelEncoder().fit_transform(df[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler=StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['Age_T', 'Attrition_T', 'BusinessTravel_T', 'DailyRate_T', 'Department_T',\n",
    "       'DistanceFromHome_T', 'Education_T', 'EducationField_T', 'EmployeeNumber_T',\n",
    "       'EnvironmentSatisfaction_T', 'Gender_T', 'HourlyRate_T', 'JobInvolvement_T',\n",
    "       'JobLevel_T', 'JobRole_T', 'JobSatisfaction_T', 'MaritalStatus_T',\n",
    "       'MonthlyIncome_T', 'MonthlyRate_T', 'NumCompaniesWorked_T', 'OverTime_T',\n",
    "       'PercentSalaryHike_T', 'PerformanceRating_T', 'RelationshipSatisfaction_T',\n",
    "       'StockOptionLevel_T', 'TotalWorkingYears_T', 'TrainingTimesLastYear_T',\n",
    "       'WorkLifeBalance_T', 'YearsAtCompany_T', 'YearsInCurrentRole_T',\n",
    "       'YearsSinceLastPromotion_T', 'YearsWithCurrManager_T']]=scaler.fit_transform(data[['Age', 'Attrition', 'BusinessTravel', 'DailyRate', 'Department',\n",
    "       'DistanceFromHome', 'Education', 'EducationField', 'EmployeeNumber',\n",
    "       'EnvironmentSatisfaction', 'Gender', 'HourlyRate', 'JobInvolvement',\n",
    "       'JobLevel', 'JobRole', 'JobSatisfaction', 'MaritalStatus',\n",
    "       'MonthlyIncome', 'MonthlyRate', 'NumCompaniesWorked', 'OverTime',\n",
    "       'PercentSalaryHike', 'PerformanceRating', 'RelationshipSatisfaction',\n",
    "       'StockOptionLevel', 'TotalWorkingYears', 'TrainingTimesLastYear',\n",
    "       'WorkLifeBalance', 'YearsAtCompany', 'YearsInCurrentRole',\n",
    "       'YearsSinceLastPromotion', 'YearsWithCurrManager']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the optimal number of clusters using elbow method\n",
    "from sklearn.cluster import KMeans\n",
    "x = data.loc[:,['Age_T','MonthlyIncome_T']].values\n",
    "WCSS = []\n",
    "for i in range(1,11):\n",
    "    model = KMeans(n_clusters = i,init = 'k-means++')\n",
    "    model.fit(x)\n",
    "    WCSS.append(model.inertia_)\n",
    "fig = plt.figure(figsize = (7,7))\n",
    "plt.plot(range(1,11),WCSS, linewidth=4, markersize=12,marker='o',color = 'green')\n",
    "plt.xticks(np.arange(11))\n",
    "plt.xlabel(\"Number of clusters\")\n",
    "plt.ylabel(\"WCSS\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans=KMeans(n_clusters=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.fit(data[['Age_T','MonthlyIncome_T']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['kmeans_4']=kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=data['Age'],y=data['MonthlyIncome'],c=data['kmeans_4'])\n",
    "plt.xlabel(\"Age \")\n",
    "plt.ylabel(\"MonthlyIncome\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML-Modelo Preditivo\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, roc_auc_score\n",
    "from sklearn.metrics import auc, roc_auc_score, roc_curve, recall_score, log_loss\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score, make_scorer\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "\n",
    "# sklearn modules for preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "# from imblearn.over_sampling import SMOTE  # SMOTE\n",
    "# sklearn modules for ML model selection\n",
    "from sklearn.model_selection import train_test_split  # import 'train_test_split'\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Libraries for data modelling\n",
    "from sklearn import svm, tree, linear_model, neighbors\n",
    "from sklearn import naive_bayes, ensemble, discriminant_analysis, gaussian_process\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Common sklearn Model Helpers\n",
    "from sklearn import feature_selection\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "# from sklearn.datasets import make_classification\n",
    "\n",
    "# sklearn modules for performance metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_recall_curve\n",
    "from sklearn.metrics import auc, roc_auc_score, roc_curve, recall_score, log_loss\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score, make_scorer\n",
    "from sklearn.metrics import average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns = ['EmployeeNumber','Over18','StandardHours'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform non-numeric columns into numerical columns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "for column in df.columns:\n",
    "        if df[column].dtype == np.number:\n",
    "            continue\n",
    "        df[column] = LabelEncoder().fit_transform(df[column])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef72b7c7",
   "metadata": {},
   "source": [
    "## Split de data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd9fb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into independent 'X' and dependent 'Y' variables\n",
    "X = df.drop('Attrition', axis=1)\n",
    "y = df.Attrition\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale fit do DF para treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 5))\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X_train.columns)\n",
    "X_test= pd.DataFrame(scaler.transform(X_test), columns = X_test.columns)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, X_train, X_test, y_train, y_test):\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    y_train_pred = model.predict(X_train)\n",
    "\n",
    "    print(\"TRAINIG RESULTS: \\n===============================\")\n",
    "    clf_report = pd.DataFrame(classification_report(y_train, y_train_pred, output_dict=True))\n",
    "    #print(f\"CONFUSION MATRIX:\\n{confusion_matrix(y_train, y_train_pred)}\")\n",
    "    print(f\"ACCURACY SCORE: {accuracy_score(y_train, y_train_pred):.4f}\")\n",
    "    print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n",
    "\n",
    "    print(\"\\n TESTING RESULTS: \\n===============================\")\n",
    "    clf_report = pd.DataFrame(classification_report(y_test, y_test_pred, output_dict=True))\n",
    "    #print(f\"CONFUSION MATRIX:\\n{confusion_matrix(y_test, y_test_pred)}\")\n",
    "    print(f\"ACCURACY SCORE: {accuracy_score(y_test, y_test_pred):.4f}\")\n",
    "    print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_clf = LogisticRegression(solver='liblinear', penalty='l1')\n",
    "lr_clf.fit(X_train, y_train)\n",
    "\n",
    "evaluate(lr_clf, X_train, X_test, y_train, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(n_estimators=50, bootstrap=False)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "evaluate(rf_clf, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf = SVC(kernel='linear')\n",
    "svm_clf.fit(X_train, y_train)\n",
    "\n",
    "evaluate(svm_clf, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "                           colsample_bytree=0.8, gamma=1.5, learning_rate=0.05,\n",
    "                           max_delta_step=0, max_depth=5, min_child_weight=7,\n",
    "                           n_estimators=200, n_jobs=-1, nthread=None,\n",
    "                           objective='binary:logistic', random_state=0, reg_alpha=0,\n",
    "                           reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "                           subsample=0.6)\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "evaluate(xgb_clf, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_clf = CatBoostClassifier()\n",
    "cb_clf.fit(X_train, y_train, verbose=0)\n",
    "\n",
    "evaluate(cb_clf, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importância de variáveis e peso no Attrition do funcionário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3800225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the feature importances (the higher, the more important the feature).\n",
    "importances = pd.DataFrame({'feature':df.iloc[:, 1:df.shape[1]].columns,'importance':np.round(forest.feature_importances_,3)}) #Note: The target column is at position 0\n",
    "importances = importances.sort_values('importance',ascending=False).set_index('feature')\n",
    "importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Esta são as variáveis que influenciam a saída do funcionário. Agora é definir a escala da variável. Por exemplo, se o MonthlyIncome aumentar, Attrition diminui. Se DistanceFromHome diminuir, Attrition diminui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7566693d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the importance\n",
    "importances.plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gráficos explicativos relacionados às variáveis importantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teste com outros modelos supervisionados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selection of algorithms to consider and set performance measure\n",
    "models = []\n",
    "models.append(('Logistic Regression', LogisticRegression(solver='liblinear', random_state=7,\n",
    "                                                         class_weight='balanced')))\n",
    "models.append(('Random Forest', RandomForestClassifier(\n",
    "    n_estimators=100, random_state=7)))\n",
    "models.append(('SVM', SVC(gamma='auto', random_state=7)))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('Decision Tree Classifier',\n",
    "               DecisionTreeClassifier(random_state=7)))\n",
    "models.append(('Gaussian NB', GaussianNB()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "acc_results = []\n",
    "auc_results = []\n",
    "names = []\n",
    "# set table to table to populate with performance results\n",
    "col = ['Algorithm', 'ROC AUC Mean', 'ROC AUC STD', \n",
    "       'Accuracy Mean', 'Accuracy STD']\n",
    "df_results = pd.DataFrame(columns=col)\n",
    "i = 0\n",
    "# evaluate each model using cross-validation\n",
    "for name, model in models:\n",
    "    kfold = model_selection.KFold(\n",
    "        n_splits=10, random_state=7, shuffle = True)  # 10-fold cross-validation\n",
    "\n",
    "    cv_acc_results = model_selection.cross_val_score(  # accuracy scoring\n",
    "        model, X_train, y_train, cv=kfold, scoring='accuracy')\n",
    "\n",
    "    cv_auc_results = model_selection.cross_val_score(  # roc_auc scoring\n",
    "        model, X_train, y_train, cv=kfold, scoring='roc_auc')\n",
    "\n",
    "    acc_results.append(cv_acc_results)\n",
    "    auc_results.append(cv_auc_results)\n",
    "    names.append(name)\n",
    "    df_results.loc[i] = [name,\n",
    "                         round(cv_auc_results.mean()*100, 2),\n",
    "                         round(cv_auc_results.std()*100, 2),\n",
    "                         round(cv_acc_results.mean()*100, 2),\n",
    "                         round(cv_acc_results.std()*100, 2)\n",
    "                         ]\n",
    "    i += 1\n",
    "df_results.sort_values(by=['ROC AUC Mean'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification Accuracy is the number of correct predictions made as a ratio of all predictions made.\n",
    "It is the most common evaluation metric for classification problems. However, it is often misused as it is only really suitable when there are an equal number of observations in each class and all predictions and prediction errors are equally important. It is not the case in this project, so a different scoring metric may be more suitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 7))\n",
    "fig.suptitle('Algorithm Accuracy Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(acc_results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Area under ROC Curve (or AUC for short) is a performance metric for binary classification problems.\n",
    "The AUC represents a model’s ability to discriminate between positive and negative classes. An area of 1.0 represents a model that made all predictions perfectly. An area of 0.5 represents a model as good as random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 7))\n",
    "fig.suptitle('Algorithm ROC AUC Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(auc_results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = model_selection.KFold(n_splits=10, random_state=7, shuffle=True)\n",
    "modelCV = LogisticRegression(solver='liblinear',\n",
    "                             class_weight=\"balanced\", \n",
    "                             random_state=7)\n",
    "scoring = 'roc_auc'\n",
    "results = model_selection.cross_val_score(\n",
    "    modelCV, X_train, y_train, cv=kfold, scoring=scoring)\n",
    "print(\"AUC score (STD): %.2f (%.2f)\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearchCV allows use to fine-tune hyper-parameters by searching over specified parameter values for an estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'C': np.arange(1e-03, 2, 0.01)} # hyper-parameter list to fine-tune\n",
    "log_gs = GridSearchCV(LogisticRegression(solver='liblinear', # setting GridSearchCV\n",
    "                                         class_weight=\"balanced\", \n",
    "                                         random_state=7),\n",
    "                      \n",
    "                      return_train_score=True,\n",
    "                      param_grid=param_grid,\n",
    "                      scoring='roc_auc',\n",
    "                      cv=10)\n",
    "\n",
    "log_grid = log_gs.fit(X_train, y_train)\n",
    "log_opt = log_grid.best_estimator_\n",
    "results = log_gs.cv_results_\n",
    "\n",
    "print('='*20)\n",
    "print(\"best params: \" + str(log_gs.best_estimator_))\n",
    "print(\"best params: \" + str(log_gs.best_params_))\n",
    "print('best score:', log_gs.best_score_)\n",
    "print('='*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Confusion Matrix\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, log_opt.predict(X_test))\n",
    "class_names=[0,1] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy of Logistic Regression Classifier on test set: {:.2f}'.format(log_opt.score(X_test, y_test)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report for the optimised Log Regression\n",
    "log_opt.fit(X_train, y_train)\n",
    "print(classification_report(y_test, log_opt.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_opt.fit(X_train, y_train) # fit optimised model to the training data\n",
    "probs = log_opt.predict_proba(X_test) # predict probabilities\n",
    "probs = probs[:, 1] # we will only keep probabilities associated with the employee leaving\n",
    "logit_roc_auc = roc_auc_score(y_test, probs) # calculate AUC score using test dataset\n",
    "print('AUC score: %.3f' % logit_roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of getting binary estimated target features (0 or 1), a probability can be associated with the predicted target.\n",
    "The output provides a first index referring to the probability that the data belong to class 0 (employee not leaving), and the second refers to the probability that the data belong to class 1 (employee leaving).\n",
    "\n",
    "The resulting AUC score is: 0.808 which is higher than that best score during the optimisation step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fine tuning\n",
    "rf_classifier = RandomForestClassifier(class_weight = \"balanced\",\n",
    "                                       random_state=7)\n",
    "param_grid = {'n_estimators': [50, 75, 100, 125, 150, 175],\n",
    "              'min_samples_split':[2,4,6,8,10],\n",
    "              'min_samples_leaf': [1, 2, 3, 4],\n",
    "              'max_depth': [5, 10, 15, 20, 25]}\n",
    "\n",
    "grid_obj = GridSearchCV(rf_classifier,\n",
    "                        \n",
    "                        return_train_score=True,\n",
    "                        param_grid=param_grid,\n",
    "                        scoring='roc_auc',\n",
    "                        cv=10)\n",
    "\n",
    "grid_fit = grid_obj.fit(X_train, y_train)\n",
    "rf_opt = grid_fit.best_estimator_\n",
    "\n",
    "print('='*20)\n",
    "print(\"best params: \" + str(grid_obj.best_estimator_))\n",
    "print(\"best params: \" + str(grid_obj.best_params_))\n",
    "print('best score:', grid_obj.best_score_)\n",
    "print('='*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = rf_opt.feature_importances_\n",
    "indices = np.argsort(importances)[::-1] # Sort feature importances in descending order\n",
    "names = [X_train.columns[i] for i in indices] # Rearrange feature names so they match the sorted feature importances\n",
    "plt.figure(figsize=(15, 7)) # Create plot\n",
    "plt.title(\"Feature Importance\") # Create plot title\n",
    "plt.bar(range(X_train.shape[1]), importances[indices]) # Add bars\n",
    "plt.xticks(range(X_train.shape[1]), names, rotation=90) # Add feature names as x-axis labels\n",
    "plt.show() # Show plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = rf_opt.feature_importances_\n",
    "df_param_coeff = pd.DataFrame(columns=['Feature', 'Coefficient'])\n",
    "for i in range(31):\n",
    "    feat = X_train.columns[i]\n",
    "    coeff = importances[i]\n",
    "    df_param_coeff.loc[i] = (feat, coeff)\n",
    "df_param_coeff.sort_values(by='Coefficient', ascending=False, inplace=True)\n",
    "df_param_coeff = df_param_coeff.reset_index(drop=True)\n",
    "df_param_coeff.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Confusion Matrix\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, rf_opt.predict(X_test))\n",
    "class_names=[0,1] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Accuracy of RandomForest Regression Classifier on test set: {:.2f}'.format(rf_opt.score(X_test, y_test)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report for the optimised RF Regression\n",
    "rf_opt.fit(X_train, y_train)\n",
    "print(classification_report(y_test, rf_opt.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_opt.fit(X_train, y_train) # fit optimised model to the training data\n",
    "probs = rf_opt.predict_proba(X_test) # predict probabilities\n",
    "probs = probs[:, 1] # we will only keep probabilities associated with the employee leaving\n",
    "rf_opt_roc_auc = roc_auc_score(y_test, probs) # calculate AUC score using test dataset\n",
    "print('AUC score: %.3f' % rf_opt_roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ROC Graph\n",
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, log_opt.predict_proba(X_test)[:,1])\n",
    "rf_fpr, rf_tpr, rf_thresholds = roc_curve(y_test, rf_opt.predict_proba(X_test)[:,1])\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Plot Logistic Regression ROC\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
    "# Plot Random Forest ROC\n",
    "plt.plot(rf_fpr, rf_tpr, label='Random Forest (area = %0.2f)' % rf_opt_roc_auc)\n",
    "# Plot Base Rate ROC\n",
    "plt.plot([0,1], [0,1],label='Base Rate' 'k--')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Graph')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "e0acaf148705ed9ed86cc5cad12259d7985e30670e5686e5f55604a9b3b84a55"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
